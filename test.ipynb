{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42647ff1",
   "metadata": {},
   "source": [
    "# The Goal\n",
    "Our general goal is to identify standing dead trees among a dataset of aerial view forests photos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d3f9a",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed2acf",
   "metadata": {},
   "source": [
    "# Deep Learning Approach #1 - Unet + VGG19 for segmentation and classification.\n",
    "We found a downstream task similar to our goal where a team of reasearchers had a series of MRI lung scans and were tasked with classifing the afflicted regions of different lung diseases including COVID-19 usin segmented regions and labelling those regions with which class of affliction. Here is the paper where they did this https://pmc.ncbi.nlm.nih.gov/articles/PMC9497601/. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e4822",
   "metadata": {},
   "source": [
    "### Import Required Packages and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import os, time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global Constraints\n",
    "# Dimensions of the image inputs into Unet after resizing\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "# Configurable in case of hardware limitations\n",
    "EPOCHS = 25\n",
    "LR = 0.001\n",
    "# Paths to the data directories\n",
    "RBD_DIR = 'USA_segmentation/RGB_images'\n",
    "NIR_DIR = 'USA_segmentation/NIR_images'\n",
    "MASK_DIR = 'USA_segmentation/masks'\n",
    "# Test proportion of full dataset\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb086d",
   "metadata": {},
   "source": [
    "### Retrieve and Split Labelled Data\n",
    "The data from our project directory has to be read and split in order to be oprocess and used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file paths from given directories and splits them into train and test sets\n",
    "# Args:\n",
    "#   rgb_dir (str): Path from current file directory containing the RGB photos\n",
    "#   nir_dir (str): Path from current file directory containing the NIR photos\n",
    "#   mask_dir (str): Path from current file directory containing the masks\n",
    "#   test_size (float): Proportion of entire dataset contributed to tests\n",
    "#   random_state (int): Random seed\n",
    "# Returns:\n",
    "#   tuple: A tuple containin two lists (train_data, test_data)\n",
    "def read_and_split_data(rgb_dir, nir_dir, mask_dir, test_size=TEST_SIZE, random_state=RANDOM_SEED):\n",
    "    # Get all RGB images sorted \n",
    "    rgb_files = sorted(glob(os.path.join(rgb_dir, \"RGB_*.png\")))\n",
    "    \n",
    "    tags = [os.path.basename(f).replace(\"RGB_\", \"\").replace(\".png\", \"\") for f in rgb_files]\n",
    "    \n",
    "    pairs = []\n",
    "    # Iterate though each identifying tag to find corresponding RGB, NIR, and masks\n",
    "    for tag in tags:\n",
    "        rgb_path = os.path.join(rgb_dir, f\"RGB_{tag}.png\")\n",
    "        nir_path = os.path.join(rgb_dir, f\"NIR_{tag}.png\")\n",
    "        mask_path = os.path.join(rgb_dir, f\"mask_{tag}.png\")\n",
    "        \n",
    "        # Check if all three corresponding files exist\n",
    "        if os.path.exists(rgb_path) and os.path.exists(nir_path) and os.path.exists(mask_path):\n",
    "            pairs.append({\n",
    "                'rgb': rgb_path,\n",
    "                'nir': nir_path,\n",
    "                'mask': mask_path,\n",
    "                'tag': tag\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping the {tag} file due to a missing file.\")\n",
    "            \n",
    "    if not pairs:\n",
    "        raise ValueError(\"No complete data pairs were found\")\n",
    "    \n",
    "    # Split the collected data pairs into training and testing sets\n",
    "    train_data, test_data = train_test_split(pairs, test_size=test_size, random_state=random_state)\n",
    "    print(f\"Total samples found: {len(pairs)}\")\n",
    "    print(f\"Train samples: {len(train_data)}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1afc7",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes a custom dataset class for loading and preprocessing the forest images\n",
    "# Args:\n",
    "#   data_pairs (list): list of dicts with each entry having paths to rgb, nir, and mask\n",
    "#   image_size (tuple): The target (width, height) for resizing images\n",
    "class ForestDataset(Dataset):\n",
    "    def __init__(self, data_pairs, image_size, transform=None):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "    # Returns the total number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    # Loads, preprocesses, and returns a single sample (input image and mask)\n",
    "    # Args:\n",
    "    #     idx (int): Index of the sample to retrieve\n",
    "    # Returns:\n",
    "    #     tuple: A tuple containing the preprocessed input tensor (4 channels)\n",
    "    #            and the mask tensor (1 channel)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_pairs[idx]\n",
    "        rgb_path = item['rgb']\n",
    "        nir_path = item['nir']\n",
    "        mask_path = item['mask']\n",
    "\n",
    "        # Load images using PIL\n",
    "        # RGB image is converted to 'RGB'\n",
    "        # NIR and mask images are converted to 'L' (grayscale)\n",
    "        rgb_image = Image.open(rgb_path).convert(\"RGB\")\n",
    "        nir_image = Image.open(nir_path).convert(\"L\")\n",
    "        mask_image = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Resize images to the specified IMAGE_SIZE\n",
    "        rgb_image = rgb_image.resize(self.image_size)\n",
    "        nir_image = nir_image.resize(self.image_size)\n",
    "        mask_image = mask_image.resize(self.image_size)\n",
    "\n",
    "        # Convert PIL images to NumPy arrays and normalize pixel values to [0, 1]\n",
    "        # RGB: (H, W, 3)\n",
    "        # NIR: (H, W)\n",
    "        # Mask: (H, W)\n",
    "        rgb_np = np.array(rgb_image).astype(np.float32) / 255.0\n",
    "        nir_np = np.array(nir_image).astype(np.float32) / 255.0\n",
    "        mask_np = np.array(mask_image).astype(np.float32) / 255.0\n",
    "\n",
    "        # Ensure the mask is strictly binary (0 or 1)\n",
    "        mask_np[mask_np > 0.5] = 1.0\n",
    "        mask_np[mask_np <= 0.5] = 0.0\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors.\n",
    "        # PyTorch expects image tensors in (C, H, W) format\n",
    "        # RGB (H, W, 3) -> (3, H, W)\n",
    "        rgb_tensor = torch.from_numpy(rgb_np).permute(2, 0, 1)\n",
    "        # NIR (H, W) -> (1, H, W) by adding a channel dimension\n",
    "        nir_tensor = torch.from_numpy(nir_np).unsqueeze(0)\n",
    "\n",
    "        # Concatenate RGB and NIR tensors along the channel dimension to create a 4-channel input\n",
    "        input_tensor = torch.cat((rgb_tensor, nir_tensor), dim=0)\n",
    "\n",
    "        # Mask tensor also needs a channel dimension (1, H, W) for the loss function\n",
    "        mask_tensor = torch.from_numpy(mask_np).unsqueeze(0)\n",
    "\n",
    "        # Apply additional transforms if provided \n",
    "        if self.transform:\n",
    "            input_tensor = self.transform(input_tensor)\n",
    "            # Apply transform to mask if it's a spatial transform like rotation/flip\n",
    "            mask_tensor = self.transform(mask_tensor)\n",
    "\n",
    "        return input_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5945be",
   "metadata": {},
   "source": [
    "###  Define and Train Unet Segmentation Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a U-Net segmentation model using the provided training data\n",
    "# Args:\n",
    "#     train_data (list): List of dictionaries containing training image paths\n",
    "#     num_epochs (int): Number of epochs to train the model\n",
    "#     batch_size (int): Number of samples per batch\n",
    "#     learning_rate (float): Learning rate for the optimizer\n",
    "#     image_size (tuple): Target image size (width, height)\n",
    "# Returns:\n",
    "#     tuple: A tuple containing the trained model and the total training time in seconds\n",
    "def train_unet_model(train_data, num_epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LR, image_size=IMAGE_SIZE):\n",
    "\n",
    "    # Determine the device to use (GPU or CPU).\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device for training: {device}\")\n",
    "\n",
    "    # Initialize the training dataset and data loader\n",
    "    # Data transformation can be added here if more augmentations are desired\n",
    "    train_dataset = ForestDataset(train_data, image_size, transform=None)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count() // 2 or 1)\n",
    "\n",
    "    # Initialize the U-Net model from segmentation_models_pytorch\n",
    "    # 'resnet34' is chosen as the encoder, pre-trained on 'imagenet' for transfer learning\n",
    "    # in_channels=4 because our input combines 3 RGB channels and 1 NIR channel\n",
    "    # classes=1 for binary segmentation (dead trees vs. background)\n",
    "    # activation=None because BCEWithLogitsLoss handles the sigmoid activation internally\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=4,\n",
    "        classes=1,\n",
    "        activation=None\n",
    "    )\n",
    "    model.to(device) # Move model to the selected device (GPU/CPU)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    # BCEWithLogitsLoss is numerically stable and recommended for binary segmentation\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "        # Iterate over batches from the training data loader\n",
    "        for inputs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs = inputs.to(device) \n",
    "            masks = masks.to(device)   \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs) \n",
    "            loss = loss_function(outputs, masks) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0) \n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset) \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time() \n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training finished in {training_time:.2f} seconds.\")\n",
    "\n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918bd3a",
   "metadata": {},
   "source": [
    "### Fine Tuning/Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35544b5",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the trained segmentation model on the test dataset.\n",
    "# Args:\n",
    "#     model (torch.nn.Module): The trained U-Net model.\n",
    "#     test_data (list): List of dictionaries containing test image paths.\n",
    "#     image_size (tuple): Target image size (width, height).\n",
    "#     batch_size (int): Number of samples per batch.\n",
    "# Returns:\n",
    "#     tuple: A tuple containing the mean IoU score and the total testing time in seconds.\n",
    "def evaluate_model(model, test_data, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device for evaluation: {device}\")\n",
    "\n",
    "    # Initialize the test dataset and data loader\n",
    "    test_dataset = ForestDataset(test_data, image_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count() // 2 or 1)\n",
    "\n",
    "    model.eval() \n",
    "    all_preds = [] \n",
    "    all_masks = [] \n",
    "\n",
    "    print(\"Starting model evaluation...\")\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Disable gradient calculation during inference to save memory and speed up computation\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            masks = masks.to(device)  \n",
    "\n",
    "            outputs = model(inputs) \n",
    "            # Apply sigmoid to outputs to get probabilities, then threshold at 0.5 for binary predictions\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            preds = (preds > 0.5).float()\n",
    "            \n",
    "            # Move predictions and masks to CPU and convert to NumPy\n",
    "            all_preds.append(preds.cpu().numpy()) \n",
    "            all_masks.append(masks.cpu().numpy()) \n",
    "\n",
    "    end_time = time.time() \n",
    "    testing_time = end_time - start_time\n",
    "    print(f\"Evaluation finished in {testing_time:.2f} seconds.\")\n",
    "\n",
    "    # Concatenate all predictions and masks into single NumPy arrays\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_masks = np.vstack(all_masks)\n",
    "\n",
    "    # Calculate IoU (Jaccard Similarity Coefficient)\n",
    "    # Small smoothing value to avoid division by zero\n",
    "    epsilon = 1e-6 \n",
    "\n",
    "    intersection = (all_preds * all_masks).sum(axis=(1, 2, 3)) # Sum over H, W, C for each image in batch\n",
    "    union = (all_preds + all_masks).sum(axis=(1, 2, 3)) - intersection\n",
    "    iou_scores = (intersection + epsilon) / (union + epsilon) # Calculate IoU for each image\n",
    "    \n",
    "    # Calculate the mean IoU across all test samples.\n",
    "    mean_iou = np.mean(iou_scores) \n",
    "    print(f\"Mean IoU (Jaccard Similarity Coefficient): {mean_iou:.4f}\")\n",
    "\n",
    "    return mean_iou, testing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a342e3",
   "metadata": {},
   "source": [
    "# Prediction Post Processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
